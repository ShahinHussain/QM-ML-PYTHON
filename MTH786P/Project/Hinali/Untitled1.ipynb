{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5515fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the diabetes dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e02e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis \n",
    "feature_names = [c for c in df.columns if c != \"Outcome\"]\n",
    "X_raw = df[feature_names].values.astype(float)\n",
    "y = df[\"Outcome\"].values.astype(int).reshape(-1, 1)\n",
    "\n",
    "# analysis of dataset\n",
    "print(\"X_raw shape:\", X_raw.shape)\n",
    "print(\"y shape    :\", y.shape)\n",
    "print(\"Features   :\", feature_names)\n",
    "np.mean(y)\n",
    "\n",
    "# target variable distribution, want to see how many patients have diabetes vs not\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.countplot(x=y.reshape(-1))\n",
    "plt.title(\"Distribution of Diabetes Outcome\")\n",
    "plt.xlabel(\"Outcome (0 = No diabetes, 1 = Diabetes)\")\n",
    "plt.ylabel(\"Number of patients\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Class proportions:\")\n",
    "print(\"No diabetes:\", np.mean(y==0))\n",
    "print(\"Diabetes   :\", np.mean(y==1))\n",
    "\n",
    "# feature distributions, seeing what medical variables look like\n",
    "df_clean = pd.DataFrame(X_clean, columns=feature_names)\n",
    "df_clean.hist(figsize=(14,10), bins=20)\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# feature distributions for diabetic vs non-diabetic\n",
    "df_plot = pd.DataFrame(X_clean, columns=feature_names)\n",
    "df_plot[\"Outcome\"] = y\n",
    "features_to_plot = [\"Glucose\", \"BMI\", \"Age\", \"Insulin\"]\n",
    "\n",
    "for f in features_to_plot:\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.histplot(data=df_plot, x=f, hue=\"Outcome\", kde=True, bins=20)\n",
    "    plt.title(f\"{f} distribution by diabetes outcome\")\n",
    "    plt.show()\n",
    "\n",
    "# correlation matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_plot.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing \"zero\" in data with the mean of nonzero elements for specific columns\n",
    "def replace_zeros_with_nonzero_mean(X, feature_names, columns_to_fix):\n",
    "    X = X.copy()\n",
    "    for col in columns_to_fix:\n",
    "        j = feature_names.index(col)\n",
    "        mask_zero = (X[:, j] == 0)\n",
    "        n_zero = np.sum(mask_zero)\n",
    "        mask_nonzero = ~mask_zero\n",
    "        mean_val = np.mean(X[mask_nonzero, j])\n",
    "        X[mask_zero, j] = mean_val\n",
    "        print(f\"{col:15s}: replaced {n_zero} zeros with mean(nonzero) = {mean_val:.3f}\")\n",
    "    return X\n",
    "\n",
    "columns_to_fix = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n",
    "X_clean = replace_zeros_with_nonzero_mean(X_raw, feature_names, columns_to_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ecb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for standardise, data matrix and split\n",
    "def standardise(data_matrix, row_of_means=None, row_of_stds=None):\n",
    "    if row_of_means is None or row_of_stds is None:\n",
    "        row_of_means = np.mean(data_matrix, axis=0)\n",
    "        centered = data_matrix - row_of_means\n",
    "        row_of_stds = np.std(centered, axis=0)\n",
    "        return centered / row_of_stds, row_of_means, row_of_stds\n",
    "    else:\n",
    "        return (data_matrix - row_of_means) / row_of_stds\n",
    "\n",
    "def linear_regression_data(data_inputs):\n",
    "    first_column = np.ones((len(data_inputs), 1))\n",
    "    return np.c_[first_column, data_inputs]\n",
    "\n",
    "def train_test_split(X, y, test_ratio=0.2, seed=123):\n",
    "    np.random.seed(seed)\n",
    "    n = len(X)\n",
    "    idx = np.random.permutation(n)\n",
    "    test_n = int(round(n * test_ratio))\n",
    "    test_idx = idx[:test_n]\n",
    "    train_idx = idx[test_n:]\n",
    "    return X[train_idx], y[train_idx], X[test_idx], y[test_idx]\n",
    "\n",
    "def classification_accuracy(predicted_labels, true_labels):\n",
    "    return np.mean(predicted_labels.reshape(-1) == true_labels.reshape(-1))\n",
    "\n",
    "X_train_raw, y_train, X_test_raw, y_test = train_test_split(X_clean, y, test_ratio=0.2, seed=123)\n",
    "\n",
    "print(\"Train shapes:\", X_train_raw.shape, y_train.shape)\n",
    "print(\"Test shapes :\", X_test_raw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise using training data only\n",
    "X_train, mu_X, std_X = standardise(X_train_raw)\n",
    "X_test = standardise(X_test_raw, mu_X, std_X)\n",
    "\n",
    "Phi_train = linear_regression_data(X_train)\n",
    "Phi_test = linear_regression_data(X_test)\n",
    "\n",
    "print(\"Phi_train shape:\", Phi_train.shape)\n",
    "print(\"Phi_test shape :\", Phi_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using logistic regression for binary data and gradient descent\n",
    "def logistic_function(inputs):\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "def model_function(data_matrix, weights):\n",
    "    return data_matrix @ weights\n",
    "\n",
    "def binary_logistic_regression_cost_function(data_matrix, data_labels, weights):\n",
    "    z = model_function(data_matrix, weights)\n",
    "    # using logaddexp for numerical stability\n",
    "    return np.mean(np.logaddexp(0, z) - data_labels * z)\n",
    "\n",
    "def binary_logistic_regression_gradient(data_matrix, data_labels, weights):\n",
    "    p = logistic_function(model_function(data_matrix, weights))\n",
    "    return data_matrix.T @ (p - data_labels) / len(data_matrix)\n",
    "\n",
    "def binary_prediction_labels(data_matrix, weights, threshold=0.5):\n",
    "    p = logistic_function(model_function(data_matrix, weights))\n",
    "    return (p > threshold).astype(int)\n",
    "\n",
    "def gradient_descent(objective, gradient, initial_weights, step_size=1.0, no_of_iterations=1000, print_output=200):\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values = [objective(weights)]\n",
    "    for k in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "        objective_values.append(objective(weights))\n",
    "        if (k + 1) % print_output == 0:\n",
    "           print(\"Iteration {k}/{m}, objective = {o}.\".format(k=k+1, m=no_of_iterations, o=objective_values[-2]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=no_of_iterations, m=no_of_iterations, o=objective_values[-1]))\n",
    "    return weights, objective_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the baseline logistic model, using step size: tau = 3.9 * s / ||Phi||^2\n",
    "\n",
    "w0 = np.zeros((Phi_train.shape[1], 1))\n",
    "step_size = 3.9 * len(Phi_train) / (np.linalg.norm(Phi_train) ** 2)\n",
    "\n",
    "objective = lambda w: binary_logistic_regression_cost_function(Phi_train, y_train, w)\n",
    "gradient = lambda w: binary_logistic_regression_gradient(Phi_train, y_train, w)\n",
    "\n",
    "w_log, objective_vals = gradient_descent(objective, gradient, w0, step_size=step_size, no_of_iterations=2000, print_output=200)\n",
    "\n",
    "pred_train = binary_prediction_labels(Phi_train, w_log)\n",
    "pred_test = binary_prediction_labels(Phi_test, w_log)\n",
    "\n",
    "acc_train = classification_accuracy(pred_train, y_train)\n",
    "acc_test = classification_accuracy(pred_test, y_test)\n",
    "\n",
    "print(\"Train accuracy:\", acc_train)\n",
    "print(\"Test accuracy :\", acc_test)\n",
    "\n",
    "plt.plot(objective_vals)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Training objective values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e681b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix, ROC graph and AUC \n",
    "def confusion_matrix_binary(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    tn = np.sum((y_true==0) & (y_pred==0))\n",
    "    fp = np.sum((y_true==0) & (y_pred==1))\n",
    "    fn = np.sum((y_true==1) & (y_pred==0))\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    return np.array([[tn, fp],[fn, tp]])\n",
    "\n",
    "def roc_curve_points(data_matrix, weights, y_true, delta=0.01):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    probs = logistic_function(model_function(data_matrix, weights)).reshape(-1)\n",
    "    thresholds = np.arange(0, 1+delta, delta)\n",
    "\n",
    "    points = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (probs > t).astype(int)\n",
    "        cm = confusion_matrix_binary(y_true, y_pred)\n",
    "        tn, fp = cm[0,0], cm[0,1]\n",
    "        fn, tp = cm[1,0], cm[1,1]\n",
    "        tpr = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        fpr = fp/(fp+tn) if (fp+tn)>0 else 0.0\n",
    "        points.append((fpr, tpr, t))\n",
    "    return points\n",
    "\n",
    "def auc_from_roc_points(roc_pts):\n",
    "    roc_sorted = sorted(roc_pts, key=lambda x: x[0])  # sort by FPR\n",
    "    fpr = np.array([p[0] for p in roc_sorted])\n",
    "    tpr = np.array([p[1] for p in roc_sorted])\n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "cm_log = confusion_matrix_binary(y_test, pred_test)\n",
    "print(\"Confusion matrix [[TN, FP],[FN, TP]]:\\n\", cm_log)\n",
    "\n",
    "roc_pts_log = roc_curve_points(Phi_test, w_log, y_test, delta=0.01)\n",
    "auc_log = auc_from_roc_points(roc_pts_log)\n",
    "print(\"Test AUC (baseline logistic):\", auc_log)\n",
    "\n",
    "x = [p[0] for p in roc_pts_log]\n",
    "y_ = [p[1] for p in roc_pts_log]\n",
    "plt.plot(x, y_)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve (baseline logistic) - test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec99b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold split and grid search\n",
    "def KFold_split(data_size, K, seed=123456789):\n",
    "    np.random.seed(seed)\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [indexes[i*m + min(i,r):(i+1)*m + min(i+1,r)] for i in range(K)]\n",
    "    return indexes_split\n",
    "\n",
    "def grid_search(objective, grid):\n",
    "    values = np.array([objective(point) for point in grid])\n",
    "    return grid[np.argmin(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5464553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1-regularised logistic regression using proximal gradient\n",
    "def soft_thresholding(argument, threshold):\n",
    "    return np.sign(argument) * np.maximum(0, np.abs(argument) - threshold)\n",
    "\n",
    "def logistic_cost_L1(data_matrix, labels, weights, alpha, penalise_bias=False):\n",
    "    base = binary_logistic_regression_cost_function(data_matrix, labels, weights)\n",
    "    if penalise_bias:\n",
    "        return base + alpha * np.sum(np.abs(weights))\n",
    "    return base + alpha * np.sum(np.abs(weights[1:]))\n",
    "\n",
    "def logistic_grad_smooth_part(data_matrix, labels, weights):\n",
    "    # gradient of just the logistic loss (smooth part)\n",
    "    return binary_logistic_regression_gradient(data_matrix, labels, weights)\n",
    "\n",
    "def proximal_gradient_descent(objective, gradient, proximal_map, initial_weights, step_size=1.0, no_of_iterations=1000, print_output=200):\n",
    "    weights = np.copy(initial_weights)\n",
    "    objective_values = [objective(weights)]\n",
    "    for k in range(no_of_iterations):\n",
    "        weights = proximal_map(weights - step_size * gradient(weights))\n",
    "        objective_values.append(objective(weights))\n",
    "        if (k + 1) % print_output == 0:\n",
    "            print(\"Iteration {k}/{m}, objective = {o}.\".format(k=k+1, m=no_of_iterations, o=objective_values[-2]))\n",
    "    print(\"Iteration completed after {k}/{m}, objective = {o}.\".format(k=no_of_iterations, m=no_of_iterations, o=objective_values[-1]))\n",
    "    return weights, objective_values\n",
    "\n",
    "def train_logistic_L1_prox(Phi, y, alpha, iters=3000, print_output=500):\n",
    "    w0 = np.zeros((Phi.shape[1], 1))\n",
    "    step = 0.9 * len(Phi) / (np.linalg.norm(Phi) ** 2)\n",
    "\n",
    "    objective = lambda w: logistic_cost_L1(Phi, y, w, alpha, penalise_bias=False)\n",
    "    gradient = lambda w: logistic_grad_smooth_part(Phi, y, w)\n",
    "\n",
    "    # proximal map\n",
    "    # no threshold for bias term\n",
    "    def prox(w):\n",
    "        w_new = np.copy(w)\n",
    "        w_new[1:] = soft_thresholding(w_new[1:], step * alpha)\n",
    "        return w_new\n",
    "\n",
    "    return proximal_gradient_descent(\n",
    "        objective, gradient, prox, w0,\n",
    "        step_size=step, no_of_iterations=iters, print_output=print_output)\n",
    "\n",
    "def kfold_cv_L1(Phi, y, K, alpha, iters=2000):\n",
    "    idx_splits = KFold_split(len(Phi), K)\n",
    "    accs = []\n",
    "    for i in range(K):\n",
    "        val_idx = idx_splits[i]\n",
    "        tr_idx = np.concatenate([idx_splits[j] for j in range(K) if j != i])\n",
    "\n",
    "        w_hat, _ = train_logistic_L1_prox(Phi[tr_idx], y[tr_idx], alpha,iters=iters, print_output=10**9)\n",
    "        pred_val = binary_prediction_labels(Phi[val_idx], w_hat)\n",
    "        accs.append(classification_accuracy(pred_val, y[val_idx]))\n",
    "    # return validation error\n",
    "    return 1.0 - np.mean(accs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation curve of validation error vs alpha (L1)\n",
    "alpha_grid = np.array([0.0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2])\n",
    "K = 5\n",
    "\n",
    "cv_errors = np.array([kfold_cv_L1(Phi_train, y_train, K, a, iters=2000) for a in alpha_grid])\n",
    "\n",
    "best_alpha = alpha_grid[np.argmin(cv_errors)]\n",
    "best_error = np.min(cv_errors)\n",
    "\n",
    "print(\"Best alpha (L1) =\", best_alpha)\n",
    "print(\"Best CV error   =\", best_error)\n",
    "\n",
    "plt.plot(alpha_grid, cv_errors, marker='o')\n",
    "plt.xlabel(\"alpha (L1)\")\n",
    "plt.ylabel(\"K-fold validation error (1 - accuracy)\")\n",
    "plt.title(\"L1 logistic regression: CV error vs alpha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training final L1 model with the best alpha\n",
    "w_l1, obj_vals_l1 = train_logistic_L1_prox(Phi_train, y_train, best_alpha,iters=4000, print_output=500)\n",
    "\n",
    "pred_test_l1 = binary_prediction_labels(Phi_test, w_l1)\n",
    "acc_test_l1 = classification_accuracy(pred_test_l1, y_test)\n",
    "\n",
    "print(\"Test accuracy (L1):\", acc_test_l1)\n",
    "\n",
    "plt.plot(obj_vals_l1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.title(\"L1 logistic regression training objective\")\n",
    "plt.show()\n",
    "\n",
    "print(\"L1 weights (bias then features):\")\n",
    "for name, val in zip([\"BIAS\"] + feature_names, w_l1.reshape(-1)):\n",
    "    print(f\"{name:25s} {val: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix, ROC and AUC for L1 model\n",
    "cm_l1 = confusion_matrix_binary(y_test, pred_test_l1)\n",
    "print(\"Confusion matrix [[TN, FP],[FN, TP]]:\\n\", cm_l1)\n",
    "\n",
    "roc_pts_l1 = roc_curve_points(Phi_test, w_l1, y_test, delta=0.01)\n",
    "auc_l1 = auc_from_roc_points(roc_pts_l1)\n",
    "print(\"Test AUC (L1):\", auc_l1)\n",
    "\n",
    "x = [p[0] for p in roc_pts_l1]\n",
    "y_ = [p[1] for p in roc_pts_l1]\n",
    "plt.plot(x, y_)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve (L1 model) - test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3adc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons\n",
    "print(\"Summary:\")\n",
    "print(\"Baseline logistic: test acc =\", acc_test,  \", test AUC =\", auc_log)\n",
    "print(\"L1 logistic      : test acc =\", acc_test_l1, \", test AUC =\", auc_l1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
