{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a485333",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td width = 40% align = \"left\">\n",
    "            <h3> MTH786 Machine Learning with Python</h3>\n",
    "        </td>\n",
    "        <td width = 35%>            \n",
    "        </td>\n",
    "        <td width = 25% align = \"left\">\n",
    "            <h3>Semester A </h3>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td width = 40% align = \"left\">\n",
    "            <h3> Coursework 6</h3>\n",
    "        </td>\n",
    "        <td width = 35%>            \n",
    "        </td>\n",
    "        <td width = 25% align = \"left\">\n",
    "            <h3>Dr Nicola Perra </h3>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.testing import assert_array_almost_equal, assert_array_equal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f18fe",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "By completing this exercise you will write a set of functions that are used for building a ridge regression for a given data samples. You will then learn how to perform K-cross validation for a hyperparameter selection. You will finish with applying the above methods to a real data, that needs to be standardised first.\n",
    "\n",
    "\n",
    "1. Implement function **ridge_regression_data** that computes (and outputs) the ridge regression data matrix $\\Phi\\left(\\mathbf{X}\\right)$ defined as\n",
    "- if a degree is given and it is larger than $1$ then the data matrix should coincide with the polynomial basis matrix\n",
    "$$\n",
    "\\Phi\\left(\\mathbf{X}\\right) = \n",
    "\\begin{pmatrix}\n",
    "1 & \\left(x^{(1)}\\right)^1 & \\left(x^{(1)}\\right)^2 & \\ldots & \\left(x^{(1)}\\right)^d \\\\\n",
    "1 & \\left(x^{(2)}\\right)^1 & \\left(x^{(2)}\\right)^2 & \\ldots & \\left(x^{(2)}\\right)^d \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "1 & \\left(x^{(s)}\\right)^1 & \\left(x^{(s)}\\right)^2 & \\ldots & \\left(x^{(s)}\\right)^d \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- otherwise, if the degree is not provided or if it is equal to $1$, then the ridge regression data matrix should coincide with the linear regression data matrix, i.e.\n",
    "$$\n",
    "\\Phi\\left(\\mathbf{X}\\right) = \n",
    "\\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\ldots & x^{(1)}_d \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\ldots & x^{(2)}_d \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "1 & x^{(s)}_1 & x^{(s)}_2 & \\ldots & x^{(s)}_d \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The function **ridge_regression_data**\n",
    "should take the $1$ compulsory argument *data_inputs* and $1$ optional argument *degree*\n",
    "- NumPy array *data_inputs* representing a list of inputs $x^{(1)}, x^{(2)}, \\ldots, x^{(s)}$. Each of $x^{(i)}$ is either a vector in the case of linear ridge regression ($d=1$) or a scalar in case of polynomial regression ($d>1$)\n",
    "- and integer *degree* representing the degree $d$ of a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_data(data_inputs, degree=1):\n",
    "\n",
    "    # this function does not produce a polynomial basis, if data_inputs.ndim>1 it assumes d=1\n",
    "    if (degree > 1 and data_inputs.ndim > 1):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # if the input is just a vector we use a polynomial regression\n",
    "    if (data_inputs.ndim == 1):\n",
    "        X_matrix = np.ones((len(data_inputs), 1))\n",
    "        for i in range(degree):\n",
    "            X_matrix = np.c_[X_matrix, np.power(data_inputs, i + 1)]\n",
    "    else:\n",
    "        first_column = np.ones((len(data_inputs), 1))\n",
    "        X_matrix = np.c_[first_column, data_inputs]\n",
    "        \n",
    "    return X_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04619a1",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([1, 2, 3, 4])\n",
    "test_degree = 2\n",
    "ridge_regression_data(test_inputs, test_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[10, 20], [12, 34], [43, 44], [44, 45]])\n",
    "ridge_regression_data(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b7f3a",
   "metadata": {},
   "source": [
    "2. Write a function **ridge_regression** that takes three arguments *data_matrix*, *data_outputs* and *regularisation*, which computes and returns the solution $\\hat{\\mathbf{W}}$ of the normal equation\n",
    "$$\n",
    "\\left(\\Phi^{\\top}\\left(\\mathbf{X}\\right)\\Phi\\left(\\mathbf{X}\\right) +\\alpha I \\right)\\hat{\\mathbf{W}} = \\Phi^{\\top}\\left(\\mathbf{X}\\right)\\mathbf{Y}\n",
    "$$\n",
    "Here $\\Phi\\left(\\mathbf{X}\\right)$  is the mathematical representation of *data_matrix*,\n",
    "$\\mathbf{Y}$ is the mathematical representation of *data_outputs* and $\\alpha$ is the mathematical representation of *regularisation*, while $\\hat{\\mathbf{W}}$ is a mathematical representation for coefficients of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f617f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(data_matrix, data_outputs, regularisation=0):\n",
    "    return np.linalg.solve(data_matrix.T @ data_matrix +regularisation * np.identity(len(data_matrix.T)),\n",
    "                           data_matrix.T @ data_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90653ea1",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43665749",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "test_outputs = np.array([0, 1, 4, 5, 8])\n",
    "plt.scatter(test_inputs,test_outputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54568bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_degree = 2\n",
    "regularisation = 0.2\n",
    "test_data_matrix = ridge_regression_data(test_inputs, test_degree)\n",
    "weights=ridge_regression(test_data_matrix, test_outputs, regularisation)\n",
    "prediction=test_data_matrix@weights\n",
    "\n",
    "plt.scatter(test_inputs,test_outputs)\n",
    "plt.scatter(test_inputs,prediction,color='Red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2cbe8",
   "metadata": {},
   "source": [
    "3. Write a function **prediction_function** that evaluates your predicted regression function at the given points $\\mathbf{X} = \n",
    "\\left\\{x^{(1)}, x^{(2)}, \\ldots, x^{(s)}\\right\\}$ for given coefficients $\\mathbf{W} = \\left(w^{(0)}, w^{(1)},\\ldots,w^{(d)}\\right)$. The function **prediction_function** takes the arguments *data_matrix* and *weights* as inputs and returns a value of the regression function evaluated for every $x \\in \\mathbf{X}$ via\n",
    "$$\n",
    "f_{\\mathbf{W}}\\left(\\mathbf{x}\\right)\n",
    "= w^{(0)}+w^{(1)}x_1+\\ldots+w^{(d)}x_d,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "f_{\\mathbf{W}}\\left(x\\right)\n",
    "= w^{(0)}+w^{(1)}x+\\ldots+w^{(d)}x^d,\n",
    "$$\n",
    "where _data_matrix_ is a NumPy array representing data matrix $\\Phi\\left(\\mathbf{X}\\right)$, _weights_ is a NumPy representation of coefficients vector $\\mathbf{W}$. The function should return a vector of the regression function values $\\left(f_{\\mathbf{W}}\\left(x^{(1)}\\right), f_{\\mathbf{w}}\\left(x^{(2)}\\right), \\ldots, f_{\\mathbf{w}}\\left(x^{(s)}\\right)\\right)^{\\top}$.\n",
    "\n",
    "*Hint:* consider matrix $\\Phi\\left(\\mathbf{X}\\right)\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ba731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(data_matrix, weights):\n",
    "    return data_matrix @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c1544",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[1, 2, 3]])\n",
    "test_weights = np.array([[-1, 1], [2, 2], [-3, 3], [4, 4]])\n",
    "test_data_matrix = ridge_regression_data(test_inputs)\n",
    "assert_array_almost_equal(prediction_function(test_data_matrix, test_weights),\n",
    "                          np.array([[7, 21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797723a",
   "metadata": {},
   "source": [
    "4. Write a function **prediction_error** that evaluates a mean-squared error over the set of data inputs and outputs. The function **prediction_error** takes the arguments _data_matrix_, _data_outputs_ and _weights_ as inputs and returns a mean squared error defined by\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{2s} \\left\\|\\Phi\\left(\\mathbf{X}\\right)\\mathbf{W} - \\mathbf{Y} \\right\\|^2,\n",
    "$$\n",
    "where $\\Phi\\left(\\mathbf{X}\\right)$ is a mathematical representation of _data_matrix_, $\\mathbf{Y}$ is a mathematical representation of _data_outputs_ and $\\mathbf{W}$ is a mathematical representation of _weights_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_error(data_matrix, data_outputs, weights):\n",
    "    return 1 / (2 * len(data_matrix)) * (np.linalg.norm(data_matrix @ weights -data_outputs))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b899f10",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "test_data_outputs = np.array([0, 1, 0, -1, 0])\n",
    "test_weights = np.array([2 / 5, -4 / 5, 6 / 5])\n",
    "test_data_matrix = ridge_regression_data(test_inputs, len(test_weights) - 1)\n",
    "\n",
    "assert_array_almost_equal(\n",
    "    prediction_error(test_data_matrix, test_data_outputs, test_weights),\n",
    "    0.359125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[1, -1], [2, 2]])\n",
    "test_data_outputs = np.array([[-1, 2], [1, 3]])\n",
    "test_data_matrix = ridge_regression_data(test_inputs)\n",
    "test_weights = np.array([[0, 0], [1, 2], [3, 4]])\n",
    "assert_array_almost_equal(\n",
    "    prediction_error(test_data_matrix, test_data_outputs, test_weights), 36.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3243f38",
   "metadata": {},
   "source": [
    "### K-fold cross validation  \n",
    "In this section you are asked to implement a number of functions that are used for a K-fold cross validation strategy as introduced in the lecture. This is a strategy for a calculation of a validation error using a smart data splitting and can be described as follows:\n",
    "1. split the data, joined inputs and outputs, into $K$ approximately equal chunks. Let us call them $D_1, D_2, \\ldots, D_K$;\n",
    "2. for every $i = 1,\\ldots,K$ evaluate optimal weights/coefficients $\\mathbf{W}_i$ for the ridge regression evaluated over the data set $D_1,D_2,\\ldots,D_{i-1},D_{i+1},\\ldots,D_K$ and a corresponding validation error $L_i$ evaluated over the set $D_i$;\n",
    "3. evaluate average of optimal weights \n",
    "$\\hat{\\mathbf{W}} = \\frac{1}{K}\\left(\\mathbf{W}_1+\\mathbf{W}_2+\\ldots+\\mathbf{W}_K\\right)$ and an average validation error \n",
    "$L = \\frac{1}{K}\\left(L_1+L_2+\\ldots+L_K\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afb1d3",
   "metadata": {},
   "source": [
    "1. Write a function **KFold_split** that takes two arguments *data_size* and *K* and outputs a random split of integer indexes $\\left[0,1,\\ldots,data\\_size\\right)$ into $K$ almost equal chunks. For example, for $K=2$ and $data\\_size = 5$\n",
    "it may return $[[3,0],[2,4,1]]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(data_size, K):\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K) # m is the quotient and r the reminder of the division data_size/K\n",
    "    indexes_split = [indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)] for i in range(K)]\n",
    "\n",
    "    return indexes_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b318fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's understand a bit better what is going on in the for loop\n",
    "K=3\n",
    "m, r = divmod(10, K)\n",
    "print (m,r)\n",
    "print ('---')\n",
    "for i in range(K):\n",
    "    print (i * m + min(i, r),(i + 1) * m + min(i + 1, r))\n",
    "# hence this effectively tries to split the data into equally sized chuncks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8c85d",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size = np.random.randint(low=1, high=10)\n",
    "test_K = np.random.randint(low=1, high=5)\n",
    "print (test_data_size,test_K )\n",
    "indexes_split = KFold_split(test_data_size, test_K)\n",
    "print (indexes_split)\n",
    "data_indexes = np.array([])\n",
    "for i in range(test_K):\n",
    "    data_indexes = np.append(data_indexes, indexes_split[i])\n",
    "data_indexes = np.sort(data_indexes)\n",
    "assert_array_almost_equal(data_indexes, np.array(range(test_data_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44452c9",
   "metadata": {},
   "source": [
    "2. Write a function **KFold_cross_validation** that takes $5$ arguments\n",
    "- *data_matrix* - a data matrix containing all inputs in an appropriate form (see **Ridge regression** section)\n",
    "- *data_outputs* - a data matrix containing all outputs in a matrix form\n",
    "- *K* - a positive integer number representing the number of chunks used for a validation algorithm\n",
    "- *model_evaluation* - a lambda-function that takes two parameters *data_matrix* and *data_outputs*, and evaluates optimal weights/coefficients/parameters of some ML model\n",
    "- *error_evaluation* - a lambda-function that takes three parameters *data_matrix*, *data_outputs*, and *weights* and evaluates a validation error.\n",
    "\n",
    "For an example of two last functions see the test below.\n",
    "\n",
    "The function **KFold_cross_validation** should return a matrix/vector of coefficients/weights and a validation error that are both evaluated as averages of optimal weights and validation error of every iteration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_cross_validation(data_matrix, data_outputs, K, model_evaluation,error_evaluation):\n",
    "\n",
    "    data_size = len(data_matrix)\n",
    "    # Partition data indicies, get K folds.\n",
    "    indexes_split = KFold_split(data_size, K)\n",
    "    # each indexes_split[j] is an array with the ids of the data matrix for that split\n",
    "    # all the others ids can be used for training and the j for validation\n",
    "    for i in range(K):\n",
    "        # hence this concatenates all the ids of the data matrix EXCEPT for those in the i split\n",
    "        # those are used for validation!\n",
    "        indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "        \n",
    "        # training the model in all the data EXCEPT for the i split\n",
    "        weights = model_evaluation(data_matrix[indexes], data_outputs[indexes]) \n",
    "        # the error is computed OUT OF SAMPLE in the i split\n",
    "        error = error_evaluation(data_matrix[indexes_split[i]],data_outputs[indexes_split[i]], weights)\n",
    "        # then the overall performance is measured as the average of the error for each split\n",
    "        # and the weights are also averaged\n",
    "        if (i == 0):\n",
    "            optimal_weights = weights / K\n",
    "            validation_error = error / K\n",
    "        else:\n",
    "            optimal_weights += weights / K\n",
    "            validation_error += error / K\n",
    "            \n",
    "    return optimal_weights, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0adfb",
   "metadata": {},
   "source": [
    "Let's see how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239dcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation = lambda data_matrix, data_outputs: ridge_regression(data_matrix, data_outputs, regularisation=0.1)\n",
    "error_evaluation = lambda data_matrix, data_outputs, weights: prediction_error(data_matrix, data_outputs, weights)\n",
    "\n",
    "inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "data_outputs = np.array([0, 1, 0, -1, 0])\n",
    "\n",
    "data_matrix=ridge_regression_data(inputs, 2)\n",
    "\n",
    "KFold_cross_validation(data_matrix, data_outputs, 3, model_evaluation,error_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a702cf0",
   "metadata": {},
   "source": [
    "### Data standardisation \n",
    "In real-world problems we usually get a raw data in the form of $s$ samples each of which is described by numeric several values corresponding to different characteristics of the object. Such a data could be highly non-uniform. The goal of applying $\\textit{standardisation}$ is to make sure different features of objects are on almost on the same scale so that each feature is equally important and make it easier to process by most ML algorithms. The result of standardisation is that the features will be rescaled to ensure the mean and the standard deviation to be $0$ and $1$, respectively. This means that for a data given by $\\mathbf{X} = \\left(\n",
    "\\left(\\mathbf{x}^{(1)}\\right)^{\\top},\\left(\\mathbf{x}^{(2)}\\right)^{\\top},\\ldots,\\left(\\mathbf{x}^{(s)}\\right)^{\\top}\n",
    "\\right) \\in \\mathbb{R}^{s\\times d}$ we define a new, rescaled data as:\n",
    "$$\n",
    "\\hat{\\mathbf{x}}^{(i)}_k = \\frac{\\mathbf{x}^{(i)}_k - \\left\\langle \\mathbf{x}_k \\right\\rangle }{\\left(\\sigma_{\\mathbf{x}}\\right)_k},\n",
    "$$\n",
    "where $\\left\\langle \\mathbf{x}_k \\right\\rangle = \\frac{1}{s}\\sum\\limits_{j=1}^s \\mathbf{x}^{(j)}_k$, and\n",
    "$\\left(\\sigma_\\mathbf{x}\\right)_k = \\sqrt{\n",
    "\t\\frac{1}{s}\\sum\\limits_{j=1}^s \\left(\\mathbf{x}^{(j)}_k-\\left\\langle \\mathbf{x}_k \\right\\rangle\\right)^2}$\n",
    "are the mean and standard deviation of data vector $\\mathbf{x}$.  \n",
    "\n",
    "Write two functions \n",
    "1. **standardise** to standardise the columns of a multi-dimensional array. The function **standardise**\ttakes the multi-dimensional array *data_matrix* as its input argument. It subtracts the means from each column and divides by the standard deviations. It returns the *standardised_matrix*, the *row_of_means* and the *row_of_standard_deviations*.\n",
    "2. **de_standardise** to de-standardise the columns of a multi-dimensional array. The function **de_standardise** reverses the above operation. It takes a *standardised_matrix*, the *row_of_means* and the *row_of_standard_deviations* as its arguments and returns a matrix for which the standardisation process is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2271e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    \n",
    "    row_of_means = np.mean(data_matrix, axis=0)\n",
    "    standardised_matrix = data_matrix - row_of_means\n",
    "    row_of_stds = np.std(data_matrix, axis=0)\n",
    "    \n",
    "    return (standardised_matrix / row_of_stds), row_of_means, row_of_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d224ad6",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b23d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "test_standardise_data_matrix = np.array([[-1.224745, -1.224745, -1.224745],\n",
    "                                         [0., 0., 0.],\n",
    "                                         [1.224745, 1.224745, 1.224745]])\n",
    "test_row_of_means = np.array([4, 5, 6])\n",
    "test_row_of_stds = np.array(np.sqrt([6, 6, 6]))\n",
    "\n",
    "test_result_standardise_data_matrix, test_result_row_of_means, test_result_row_of_stds = standardise(\n",
    "    test_data_matrix)\n",
    "assert_array_almost_equal(test_result_standardise_data_matrix,\n",
    "                          test_standardise_data_matrix)\n",
    "assert_array_almost_equal(test_result_row_of_means, test_row_of_means)\n",
    "assert_array_almost_equal(test_result_row_of_stds, test_row_of_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_standardise(standardised_matrix, row_of_means, row_of_stds):\n",
    "    matrix = np.copy(standardised_matrix * row_of_stds)\n",
    "    return matrix + row_of_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87eedee",
   "metadata": {},
   "source": [
    "### Boston housing price data\n",
    "Finally, we are going to apply all our tools for training a model predicting housing prices in Boston. In this exercise you will work with a real housing price data. The assignment folder contains house_prices.csv file which you will need to read the data from. This file contains an information about $N = 1200$ houses. The data columns are:\n",
    "- $\\texttt{StreetLength}$ - length of the street in front of the building\n",
    "- $\\texttt{Area}$ - total area of the lot\n",
    "- $\\texttt{Quality}$ - quality of building materials\n",
    "- $\\texttt{Condition}$ - condition of the building\n",
    "- $\\texttt{BasementArea}$ - area of the basement\n",
    "- $\\texttt{LivingArea}$ - total living area\n",
    "- $\\texttt{GarageArea}$ - a garage area\t\t\n",
    "- $\\texttt{SalePrice}$ - sale price\n",
    "\n",
    "Your task would be to build a ridge regression using $K$-fold cross validation strategy for validation and a grid search strategy for optimisation of the hyperparameter $\\alpha$ value.\n",
    "\n",
    "1. We start by loading the data. Please run the below cell to read the data from a .csv file. Make sure the .csv file is in the same folder with your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dataset_path = \"house_prices.csv\"\n",
    "housing_data = np.genfromtxt(housing_dataset_path,\n",
    "                             delimiter=\",\",\n",
    "                             skip_header=1,\n",
    "                             usecols=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "housing_data_input = housing_data[:, 0:7]\n",
    "housing_data_output = housing_data[:, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7d0bd",
   "metadata": {},
   "source": [
    "2. We then prepare the data for an analysis. You will need to standardise the inputs and outputs using the functions you developed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2033d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardised_housing_data_input, means_housing_data_input,stds_housing_data_input = standardise(housing_data_input)\n",
    "standardised_housing_data_output, means_housing_data_output,stds_housing_data_output = standardise(housing_data_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ff1b9",
   "metadata": {},
   "source": [
    "3. Write a function **grid_search** that performs a search for a minimum value of a given function on a given grid points. You function should have a signature *grid_search (objective, grid)*, where *objective* is a lambda-function to minimise and *grid* is a list of grid points. The function should return the grid point with the minimal value of objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(objective, grid):\n",
    "    values = np.array([])\n",
    "    for point in grid:\n",
    "        values = np.append(values, objective(point))\n",
    "    return grid[np.argmin(values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7460dc",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function = lambda xy: xy[0]**2 + xy[1]**2 - 2 * xy[0] * xy[1]\n",
    "test_grid = [(0, 1), (1, 0), (2, 3), (5, 5)]\n",
    "assert_array_almost_equal(grid_search(test_function, test_grid), (5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2894a",
   "metadata": {},
   "source": [
    "4. Implement a grid search algorithm to find an unknown hyperparameter $\\hat \\alpha$ such that\n",
    "$$\n",
    "\\hat{\\alpha} = \\arg\\min\\limits_{\\alpha\\geq 0} \\mathrm{Val}\\left( \\mathbf{W}_{\\alpha}\\right),\n",
    "$$\n",
    "where the validation error $\\mathrm{Val}\\left( \\mathbf{W}_{\\alpha}\\right)$ is evaluated using K-fold cross validation. Take $K=5$. Print out an optimal coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f641d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "alpha_grid = np.append(np.array([i * 0.05 for i in range(20)]),\n",
    "                       np.array([i for i in range(1, 20)]))\n",
    "\n",
    "\n",
    "error_evaluation = lambda data_matrix, data_outputs, weights: prediction_error(data_matrix, data_outputs, weights)\n",
    "\n",
    "# the objective function is the validation error computed for each alpha value in the grid below\n",
    "validation_error = lambda alpha: KFold_cross_validation(standardised_housing_data_input,\n",
    "                                                        standardised_housing_data_output, \n",
    "                                                        K,lambda data_matrix,\n",
    "                                                        data_outputs: ridge_regression(data_matrix,\n",
    "                                                                                       data_outputs,\n",
    "                                                                                       regularisation=alpha),\n",
    "                                                        error_evaluation)[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimal_alpha = grid_search(validation_error, alpha_grid)\n",
    "\n",
    "optimal_weights = KFold_cross_validation(standardised_housing_data_input,\n",
    "                                         standardised_housing_data_output, \n",
    "                                         K,lambda data_matrix,\n",
    "                                         data_outputs: ridge_regression(data_matrix, \n",
    "                                                                        data_outputs,\n",
    "                                                                        regularisation =optimal_alpha),\n",
    "                                         error_evaluation)[0]\n",
    "\n",
    "print(\n",
    "\"An optimal value of regularisation parameter is {}.\\nFor this value of regularisation parameter one gets optimal weights of the form \\n{}\".format(optimal_alpha, optimal_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get the names of the features from the data\n",
    "# they are the header of the file\n",
    "a=open(housing_dataset_path,'r')\n",
    "for i in a:\n",
    "    features=i.strip().split(',')\n",
    "    break\n",
    "a.close()\n",
    "# we need to delete the last since that is the target!\n",
    "features.pop()\n",
    "\n",
    "# now we can get the order of the weights in terms of their values\n",
    "c=0\n",
    "list_features=[]\n",
    "for i in optimal_weights:\n",
    "    list_features.append([i,features[c]])\n",
    "    c+=1\n",
    "\n",
    "list_features=sorted(list_features,reverse=True)\n",
    "for i in list_features:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36dbeb",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ee6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bootstrap we need to select N samples from the data with replacement\n",
    "# this means that we can just get N random numbers from 0 to len(data)-1 \n",
    "# indeed in this type of sampling the probability that one will be selected is constant 1/len(data)\n",
    "# we can pick N as 90% of the total sample size\n",
    "import random as rd\n",
    "data_size=len(standardised_housing_data_output)\n",
    "fraction=0.9\n",
    "samples_size=int(data_size*fraction)\n",
    "\n",
    "sample_input_list=[]\n",
    "sample_output_list=[]\n",
    "for i in range(samples_size):\n",
    "    id_random=rd.randint(0,samples_size-1)\n",
    "    sample_input_list.append(standardised_housing_data_input[id_random])\n",
    "    sample_output_list.append(standardised_housing_data_output[id_random])\n",
    "    \n",
    "sample_input=np.array(sample_input_list)\n",
    "sample_output=np.array(sample_output_list)\n",
    "\n",
    "# now can can create a function that does this plus for each sample compute the regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063db0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "def bootstrap_regression(standardised_data_input,standardised_data_output,fraction,M,alpha):\n",
    "    # first we need to know what is N: the number of samples to extract\n",
    "    data_size=len(standardised_data_output)\n",
    "    samples_size=int(data_size*fraction)\n",
    "\n",
    "    w_list=[]\n",
    "    # then for each of the M extract\n",
    "    for j in range(M):\n",
    "        sample_input_list=[]\n",
    "        sample_output_list=[]\n",
    "        for i in range(samples_size):\n",
    "            # we take N samples extract random numbers which are the id of the arrays that store the data\n",
    "            id_random=rd.randint(0,samples_size-1)\n",
    "            # note that we need to keep the X and Y correspondence hence the id_random is the same for each\n",
    "            sample_input_list.append(standardised_data_input[id_random])\n",
    "            sample_output_list.append(standardised_data_output[id_random])\n",
    "\n",
    "        # convert the list to arrays\n",
    "        sample_input=np.array(sample_input_list)\n",
    "        sample_output=np.array(sample_output_list)\n",
    "\n",
    "        # apply the regression, note that alpha is selected before with the model selection\n",
    "        weights=ridge_regression(sample_input, sample_output, regularisation=alpha)\n",
    "        # append the fitted values of Ws for the N samples in list\n",
    "        w_list.append(weights)\n",
    "    return w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=10000\n",
    "# note how the alpha is selected in the model selection before!\n",
    "w_list=bootstrap_regression(standardised_housing_data_input,standardised_housing_data_output,0.9,M,optimal_alpha)\n",
    "\n",
    "# now we have a list with all the values of w for each of the M samples\n",
    "# we can create a dictionary to get the values for each features\n",
    "dict_w={}\n",
    "for i in range(len(features)):\n",
    "    dict_w[i]=[]\n",
    "\n",
    "# append the values to each feature as list\n",
    "for i in range(M):\n",
    "    w=w_list[i]\n",
    "    for j in range(len(w)):\n",
    "        dict_w[j].append(w[j])\n",
    "        \n",
    "for i in dict_w:\n",
    "    dict_w[i]=np.array(dict_w[i]) # first convert the list into a nunpy array then we can manipulate them\n",
    "    print (features[i],np.mean(dict_w[i])) # get for example the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then use seaborn to plot, this requires pandas\n",
    "# let us first create the data for the df\n",
    "# this function takes the features and the list of w from the bootstrap, it write the data into a file for simplicity\n",
    "import pandas as pd\n",
    "\n",
    "def get_df(features,w_list):\n",
    "    # let's first create the headers of the file\n",
    "    a=open('data_for_plot.csv','w')\n",
    "    for j in range(len(features)-1):\n",
    "        a.write(features[j]+',')\n",
    "    a.write(features[-1]+'\\n')\n",
    "\n",
    "    # then we can write the data\n",
    "    for j in w_list:\n",
    "        for i in range(len(j)-1):\n",
    "            a.write(str(j[i])+',')\n",
    "        a.write(str(j[-1])+'\\n')\n",
    "    a.close()\n",
    "    df=pd.read_csv('data_for_plot.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=get_df(features,w_list) # get the dataframe\n",
    "\n",
    "\n",
    "sns.boxplot(data=df) # make the plot with seaborn\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cf58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
