{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a485333",
   "metadata": {},
   "source": [
    "<table width=100%>\n",
    "    <tr>\n",
    "        <td width = 40% align = \"left\">\n",
    "            <h3> MTH786 Machine Learning with Python</h3>\n",
    "        </td>\n",
    "        <td width = 35%>            \n",
    "        </td>\n",
    "        <td width = 25% align = \"left\">\n",
    "            <h3>Semester A </h3>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td width = 40% align = \"left\">\n",
    "            <h3> Coursework 6</h3>\n",
    "        </td>\n",
    "        <td width = 35%>            \n",
    "        </td>\n",
    "        <td width = 25% align = \"left\">\n",
    "            <h3>Dr Nicola Perra </h3>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a1d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sami/miniforge3/envs/ML/lib/python3.11/site-packages/numpy/_core/getlimits.py:552: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.testing import assert_array_almost_equal, assert_array_equal\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f18fe",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "By completing this exercise you will write a set of functions that are used for building a ridge regression for a given data samples. You will then learn how to perform K-cross validation for a hyperparameter selection. You will finish with applying the above methods to a real data, that needs to be standardised first.\n",
    "\n",
    "\n",
    "1. Implement function **ridge_regression_data** that computes (and outputs) the ridge regression data matrix $\\Phi\\left(\\mathbf{X}\\right)$ defined as\n",
    "- if a degree is given and it is larger than $1$ then the data matrix should coincide with the polynomial basis matrix\n",
    "$$\n",
    "\\Phi\\left(\\mathbf{X}\\right) = \n",
    "\\begin{pmatrix}\n",
    "1 & \\left(x^{(1)}\\right)^1 & \\left(x^{(1)}\\right)^2 & \\ldots & \\left(x^{(1)}\\right)^d \\\\\n",
    "1 & \\left(x^{(2)}\\right)^1 & \\left(x^{(2)}\\right)^2 & \\ldots & \\left(x^{(2)}\\right)^d \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "1 & \\left(x^{(s)}\\right)^1 & \\left(x^{(s)}\\right)^2 & \\ldots & \\left(x^{(s)}\\right)^d \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- otherwise, if the degree is not provided or if it is equal to $1$, then the ridge regression data matrix should coincide with the linear regression data matrix, i.e.\n",
    "$$\n",
    "\\Phi\\left(\\mathbf{X}\\right) = \n",
    "\\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\ldots & x^{(1)}_d \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\ldots & x^{(2)}_d \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "1 & x^{(s)}_1 & x^{(s)}_2 & \\ldots & x^{(s)}_d \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The function **ridge_regression_data**\n",
    "should take the $1$ compulsory argument *data_inputs* and $1$ optional argument *degree*\n",
    "- NumPy array *data_inputs* representing a list of inputs $x^{(1)}, x^{(2)}, \\ldots, x^{(s)}$. Each of $x^{(i)}$ is either a vector in the case of linear ridge regression ($d=1$) or a scalar in case of polynomial regression ($d>1$)\n",
    "- and integer *degree* representing the degree $d$ of a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ba602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_data(data_inputs, degree=1):\n",
    "    X = np.ones((len(data_inputs), 1))\n",
    "    for i in range(degree):\n",
    "        X = np.c_[X, np.power(data_inputs, i + 1)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04619a1",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72be9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([1, 2, 3, 4])\n",
    "test_degree = 2\n",
    "assert_array_equal(ridge_regression_data(test_inputs, test_degree),\n",
    "                   np.array([[1, 1, 1], [1, 2, 4], [1, 3, 9], [1, 4, 16]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf61232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "assert_array_equal(ridge_regression_data(test_inputs),\n",
    "                   np.array([[1, 1, 2], [1, 2, 3], [1, 3, 4], [1, 4, 5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b7f3a",
   "metadata": {},
   "source": [
    "2. Write a function **ridge_regression** that takes three arguments *data_matrix*, *data_outputs* and *regularisation*, which computes and returns the solution $\\hat{\\mathbf{W}}$ of the normal equation\n",
    "$$\n",
    "\\left(\\Phi^{\\top}\\left(\\mathbf{X}\\right)\\Phi\\left(\\mathbf{X}\\right) +\\alpha I \\right)\\hat{\\mathbf{W}} = \\Phi^{\\top}\\left(\\mathbf{X}\\right)\\mathbf{Y}\n",
    "$$\n",
    "Here $\\Phi\\left(\\mathbf{X}\\right)$  is the mathematical representation of *data_matrix*,\n",
    "$\\mathbf{Y}$ is the mathematical representation of *data_outputs* and $\\alpha$ is the mathematical representation of *regularisation*, while $\\hat{\\mathbf{W}}$ is a mathematical representation for coefficients of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f617f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(data_matrix, data_outputs, regularisation=0):\n",
    "    return np.linalg.solve((data_matrix.T @ data_matrix) \\\n",
    "                            + regularisation * np.eye(len(data_matrix.T @ data_matrix)),\n",
    "                              data_matrix.T @ data_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90653ea1",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54568bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "test_outputs = np.array([0, 1, 0, -1, 0])\n",
    "test_degree = 2\n",
    "regularisation = 0.2\n",
    "test_data_matrix = ridge_regression_data(test_inputs, test_degree)\n",
    "assert_array_almost_equal(\n",
    "    ridge_regression(test_data_matrix, test_outputs, regularisation),\n",
    "    np.array([0.274302, -0.346361, -0.298915]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eabafe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[0.98], [1.02]])\n",
    "test_outputs = np.array([[-0.1], [0.3]])\n",
    "test_data_matrix = ridge_regression_data(test_inputs)\n",
    "regularisation = 0.5\n",
    "assert_array_almost_equal(\n",
    "    ridge_regression(test_data_matrix, test_outputs, regularisation),\n",
    "    np.array([[0.037371],\n",
    "       [0.053286]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2cbe8",
   "metadata": {},
   "source": [
    "3. Write a function **prediction_function** that evaluates your predicted regression function at the given points $\\mathbf{X} = \n",
    "\\left\\{x^{(1)}, x^{(2)}, \\ldots, x^{(s)}\\right\\}$ for given coefficients $\\mathbf{W} = \\left(w^{(0)}, w^{(1)},\\ldots,w^{(d)}\\right)$. The function **prediction_function** takes the arguments *data_matrix* and *weights* as inputs and returns a value of the regression function evaluated for every $x \\in \\mathbf{X}$ via\n",
    "$$\n",
    "f_{\\mathbf{W}}\\left(\\mathbf{x}\\right)\n",
    "= w^{(0)}+w^{(1)}x_1+\\ldots+w^{(d)}x_d,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "f_{\\mathbf{W}}\\left(x\\right)\n",
    "= w^{(0)}+w^{(1)}x+\\ldots+w^{(d)}x^d,\n",
    "$$\n",
    "where _data_matrix_ is a NumPy array representing data matrix $\\Phi\\left(\\mathbf{X}\\right)$, _weights_ is a NumPy representation of coefficients vector $\\mathbf{W}$. The function should return a vector of the regression function values $\\left(f_{\\mathbf{W}}\\left(x^{(1)}\\right), f_{\\mathbf{w}}\\left(x^{(2)}\\right), \\ldots, f_{\\mathbf{w}}\\left(x^{(s)}\\right)\\right)^{\\top}$.\n",
    "\n",
    "*Hint:* consider matrix $\\Phi\\left(\\mathbf{X}\\right)\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a9ba731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_function(data_matrix, weights):\n",
    "    return data_matrix @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c1544",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b3a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "test_weights = np.array([2 / 5, -4 / 5, 6 / 5])\n",
    "test_data_matrix = ridge_regression_data(test_inputs, len(test_weights) - 1)\n",
    "assert_array_almost_equal(prediction_function(test_data_matrix, test_weights),\n",
    "                          np.array([0.4, 0.275, 0.3, 0.475, 0.8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4fa7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[1, 2, 3]])\n",
    "test_weights = np.array([[-1, 1], [2, 2], [-3, 3], [4, 4]])\n",
    "test_data_matrix = ridge_regression_data(test_inputs)\n",
    "assert_array_almost_equal(prediction_function(test_data_matrix, test_weights),\n",
    "                          np.array([[7, 21]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797723a",
   "metadata": {},
   "source": [
    "4. Write a function **prediction_error** that evaluates a mean-squared error over the set of data inputs and outputs. The function **prediction_error** takes the arguments _data_matrix_, _data_outputs_ and _weights_ as inputs and returns a mean squared error defined by\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{2s} \\left\\|\\Phi\\left(\\mathbf{X}\\right)\\mathbf{W} - \\mathbf{Y} \\right\\|^2,\n",
    "$$\n",
    "where $\\Phi\\left(\\mathbf{X}\\right)$ is a mathematical representation of _data_matrix_, $\\mathbf{Y}$ is a mathematical representation of _data_outputs_ and $\\mathbf{W}$ is a mathematical representation of _weights_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e78d17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_error(data_matrix, data_outputs, weights):\n",
    "    s = len(data_matrix)\n",
    "    inside = (data_matrix @ weights) - data_outputs\n",
    "    return 1/(2*s) * np.linalg.norm(inside)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b899f10",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2177aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([0, 1 / 4, 1 / 2, 3 / 4, 1])\n",
    "test_data_outputs = np.array([0, 1, 0, -1, 0])\n",
    "test_weights = np.array([2 / 5, -4 / 5, 6 / 5])\n",
    "test_data_matrix = ridge_regression_data(test_inputs, len(test_weights) - 1)\n",
    "\n",
    "assert_array_almost_equal(\n",
    "    prediction_error(test_data_matrix, test_data_outputs, test_weights),\n",
    "    0.359125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc7c6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.array([[1, -1], [2, 2]])\n",
    "test_data_outputs = np.array([[-1, 2], [1, 3]])\n",
    "test_data_matrix = ridge_regression_data(test_inputs)\n",
    "test_weights = np.array([[0, 0], [1, 2], [3, 4]])\n",
    "assert_array_almost_equal(\n",
    "    prediction_error(test_data_matrix, test_data_outputs, test_weights), 36.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3243f38",
   "metadata": {},
   "source": [
    "### K-fold cross validation  \n",
    "In this section you are asked to implement a number of functions that are used for a K-fold cross validation strategy as introduced in the lecture. This is a strategy for a calculation of a validation error using a smart data splitting and can be described as follows:\n",
    "1. split the data, joined inputs and outputs, into $K$ approximately equal chunks. Let us call them $D_1, D_2, \\ldots, D_K$;\n",
    "2. for every $i = 1,\\ldots,K$ evaluate optimal weights/coefficients $\\mathbf{W}_i$ for the ridge regression evaluated over the data set $D_1,D_2,\\ldots,D_{i-1},D_{i+1},\\ldots,D_K$ and a corresponding validation error $L_i$ evaluated over the set $D_i$;\n",
    "3. evaluate average of optimal weights \n",
    "$\\hat{\\mathbf{W}} = \\frac{1}{K}\\left(\\mathbf{W}_1+\\mathbf{W}_2+\\ldots+\\mathbf{W}_K\\right)$ and an average validation error \n",
    "$L = \\frac{1}{K}\\left(L_1+L_2+\\ldots+L_K\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afb1d3",
   "metadata": {},
   "source": [
    "1. Write a function **KFold_split** that takes two arguments *data_size* and *K* and outputs a random split of integer indexes $\\left[0,1,\\ldots,data\\_size\\right)$ into $K$ almost equal chunks. For example, for $K=2$ and $data\\_size = 5$\n",
    "it may return $[[3,0],[2,4,1]]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb977715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 1, 7, 4, 5, 6, 0, 3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size. take length\n",
    "# data size = 10, K = 3: 3, 3, 4\n",
    "\n",
    "StudentID = 140474758\n",
    "\n",
    "def KFold_split(data_size, K):\n",
    "    np.random.seed(StudentID)\n",
    "    \n",
    "    indexes = np.random.permutation(data_size)\n",
    "    base = data_size // K\n",
    "    extra = data_size % K\n",
    "\n",
    "    splits = []\n",
    "    start = 0\n",
    "    for i in range(K):\n",
    "        fold_size = base + (1 if i < extra else 0)\n",
    "        end = start + fold_size\n",
    "        splits.append(indexes[start:end])\n",
    "        start = end\n",
    "\n",
    "    return splits\n",
    "\n",
    "\"\"\"\n",
    "Simpler:\n",
    "\n",
    "def kfold_split(data_size, k):\n",
    "    indices = np.random.permutation(data_size)\n",
    "    return np.array_split(indices, k)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8c85d",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size = np.random.randint(low=100, high=1000)\n",
    "test_K = np.random.randint(low=2, high=10)\n",
    "indexes_split = KFold_split(test_data_size, test_K)\n",
    "data_indexes = np.array([])\n",
    "for i in range(test_K):\n",
    "    data_indexes = np.append(data_indexes, indexes_split[i])\n",
    "data_indexes = np.sort(data_indexes)\n",
    "assert_array_almost_equal(data_indexes, np.array(range(test_data_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44452c9",
   "metadata": {},
   "source": [
    "2. Write a function **KFold_cross_validation** that takes $5$ arguments\n",
    "- *data_matrix* - a data matrix containing all inputs in an appropriate form (see **Ridge regression** section)\n",
    "- *data_outputs* - a data matrix containing all outputs in a matrix form\n",
    "- *K* - a positive integer number representing the number of chunks used for a validation algorithm\n",
    "- *model_evaluation* - a lambda-function that takes two parameters *data_matrix* and *data_outputs*, and evaluates optimal weights/coefficients/parameters of some ML model\n",
    "- *error_evaluation* - a lambda-function that takes three parameters *data_matrix*, *data_outputs*, and *weights* and evaluates a validation error.\n",
    "\n",
    "For an example of two last functions see the test below.\n",
    "\n",
    "The function **KFold_cross_validation** should return a matrix/vector of coefficients/weights and a validation error that are both evaluated as averages of optimal weights and validation error of every iteration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_cross_validation(data_matrix, data_outputs, K, model_evaluation,\n",
    "                           error_evaluation):\n",
    "    data_size = len(data_matrix)\n",
    "    \n",
    "\n",
    "    \n",
    "    return optimal_weights, validation_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0adfb",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239dcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation = lambda data_matrix, data_outputs: ridge_regression(\n",
    "    data_matrix, data_outputs, regularisation=0.1)\n",
    "error_evaluation = lambda data_matrix, data_outputs, weights: prediction_error(\n",
    "    data_matrix, data_outputs, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a702cf0",
   "metadata": {},
   "source": [
    "### Data standardisation \n",
    "In real-world problems we usually get a raw data in the form of $s$ samples each of which is described by numeric several values corresponding to different characteristics of the object. Such a data could be highly non-uniform. The goal of applying $\\textit{standardisation}$ is to make sure different features of objects are on almost on the same scale so that each feature is equally important and make it easier to process by most ML algorithms. The result of standardisation is that the features will be rescaled to ensure the mean and the standard deviation to be $0$ and $1$, respectively. This means that for a data given by $\\mathbf{X} = \\left(\n",
    "\\left(\\mathbf{x}^{(1)}\\right)^{\\top},\\left(\\mathbf{x}^{(2)}\\right)^{\\top},\\ldots,\\left(\\mathbf{x}^{(s)}\\right)^{\\top}\n",
    "\\right) \\in \\mathbb{R}^{s\\times d}$ we define a new, rescaled data as:\n",
    "$$\n",
    "\\hat{\\mathbf{x}}^{(i)}_k = \\frac{\\mathbf{x}^{(i)}_k - \\left\\langle \\mathbf{x}_k \\right\\rangle }{\\left(\\sigma_{\\mathbf{x}}\\right)_k},\n",
    "$$\n",
    "where $\\left\\langle \\mathbf{x}_k \\right\\rangle = \\frac{1}{s}\\sum\\limits_{j=1}^s \\mathbf{x}^{(j)}_k$, and\n",
    "$\\left(\\sigma_\\mathbf{x}\\right)_k = \\sqrt{\n",
    "\t\\frac{1}{s}\\sum\\limits_{j=1}^s \\left(\\mathbf{x}^{(j)}_k-\\left\\langle \\mathbf{x}_k \\right\\rangle\\right)^2}$\n",
    "are the mean and standard deviation of data vector $\\mathbf{x}$.  \n",
    "\n",
    "Write two functions \n",
    "1. **standardise** to standardise the columns of a multi-dimensional array. The function **standardise**\ttakes the multi-dimensional array *data_matrix* as its input argument. It subtracts the means from each column and divides by the standard deviations. It returns the *standardised_matrix*, the *row_of_means* and the *row_of_standard_deviations*.\n",
    "2. **de_standardise** to de-standardise the columns of a multi-dimensional array. The function **de_standardise** reverses the above operation. It takes a *standardised_matrix*, the *row_of_means* and the *row_of_standard_deviations* as its arguments and returns a matrix for which the standardisation process is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2271e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d224ad6",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b23d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "test_standardise_data_matrix = np.array([[-1.224745, -1.224745, -1.224745],\n",
    "                                         [0., 0., 0.],\n",
    "                                         [1.224745, 1.224745, 1.224745]])\n",
    "test_row_of_means = np.array([4, 5, 6])\n",
    "test_row_of_stds = np.array(np.sqrt([6, 6, 6]))\n",
    "\n",
    "test_result_standardise_data_matrix, test_result_row_of_means, test_result_row_of_stds = standardise(\n",
    "    test_data_matrix)\n",
    "assert_array_almost_equal(test_result_standardise_data_matrix,\n",
    "                          test_standardise_data_matrix)\n",
    "assert_array_almost_equal(test_result_row_of_means, test_row_of_means)\n",
    "assert_array_almost_equal(test_result_row_of_stds, test_row_of_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_standardise(standardised_matrix, row_of_means, row_of_stds):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43a296",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87eedee",
   "metadata": {},
   "source": [
    "### Boston housing price data\n",
    "Finally, we are going to apply all our tools for training a model predicting housing prices in Boston. In this exercise you will work with a real housing price data. The assignment folder contains house_prices.csv file which you will need to read the data from. This file contains an information about $N = 1200$ houses. The data columns are:\n",
    "- $\\texttt{StreetLength}$ - length of the street in front of the building\n",
    "- $\\texttt{Area}$ - total area of the lot\n",
    "- $\\texttt{Quality}$ - quality of building materials\n",
    "- $\\texttt{Condition}$ - condition of the building\n",
    "- $\\texttt{BasementArea}$ - area of the basement\n",
    "- $\\texttt{LivingArea}$ - total living area\n",
    "- $\\texttt{GarageArea}$ - a garage area\t\t\n",
    "- $\\texttt{SalePrice}$ - sale price\n",
    "\n",
    "Your task would be to build a ridge regression using $K$-fold cross validation strategy for validation and a grid search strategy for optimisation of the hyperparameter $\\alpha$ value.\n",
    "\n",
    "1. We start by loading the data. Please run the below cell to read the data from a .csv file. Make sure the .csv file is in the same folder with your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_dataset_path = \"house_prices.csv\"\n",
    "housing_data = np.genfromtxt(housing_dataset_path,\n",
    "                             delimiter=\",\",\n",
    "                             skip_header=1,\n",
    "                             usecols=[0, 1, 2, 3, 4, 5, 6, 7])\n",
    "housing_data_input = housing_data[:, 0:7]\n",
    "housing_data_output = housing_data[:, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7d0bd",
   "metadata": {},
   "source": [
    "2. We then prepare the data for an analysis. You will need to standardise the inputs and outputs using the functions you developed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2033d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ff1b9",
   "metadata": {},
   "source": [
    "3. Write a function **grid_search** that performs a search for a minimum value of a given function on a given grid points. You function should have a signature *grid_search (objective, grid)*, where *objective* is a lambda-function to minimise and *grid* is a list of grid points. The function should return the grid point with the minimal value of objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(objective, grid):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7460dc",
   "metadata": {},
   "source": [
    "Test your function with the following unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_function = lambda xy: xy[0]**2 + xy[1]**2 - 2 * xy[0] * xy[1]\n",
    "test_grid = [(0, 1), (1, 0), (2, 3), (5, 5)]\n",
    "assert_array_almost_equal(grid_search(test_function, test_grid), (5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2894a",
   "metadata": {},
   "source": [
    "4. Implement a grid search algorithm to find an unknown hyperparameter $\\hat \\alpha$ such that\n",
    "$$\n",
    "\\hat{\\alpha} = \\arg\\min\\limits_{\\alpha\\geq 0} \\mathrm{Val}\\left( \\mathbf{W}_{\\alpha}\\right),\n",
    "$$\n",
    "where the validation error $\\mathrm{Val}\\left( \\mathbf{W}_{\\alpha}\\right)$ is evaluated using K-fold cross validation. Take $K=5$. Print out an optimal coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f641d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "alpha_grid = np.append(np.array([i * 0.05 for i in range(20)]),\n",
    "                       np.array([i for i in range(1, 20)]))\n",
    "#below you need to evaluate optimal_alpha and optimal_weights corresponding to optimal_alpha\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(\n",
    "    \"An optimal value of regularisation parameter is {}.\\nFor this value of regularisation parameter one gets optimal weights of the form \\n{}\"\n",
    "    .format(optimal_alpha, optimal_weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
